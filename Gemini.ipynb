{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till API Exhausted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set the GEMINI API key\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"\"  # Replace with your actual API key\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Function to call the Gemini API for code generation\n",
    "def call_gemini_api(prompt):\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Gemini API: {e}\")\n",
    "        return \"\"  # Return an empty string or handle the error appropriately\n",
    "\n",
    "# Function to generate code using the Gemini API\n",
    "def generate_code(natural_language_desc):\n",
    "    prompt = f\"Generate Java code for the following task:\\nDescription: {natural_language_desc}\"\n",
    "    return call_gemini_api(prompt)\n",
    "\n",
    "# Function to automatically locate the dataset\n",
    "def locate_dataset(dataset_name=\"concode/test.json\"):\n",
    "    current_dir = os.getcwd()\n",
    "    dataset_path = os.path.join(current_dir, dataset_name)\n",
    "    \n",
    "    if os.path.exists(dataset_path):\n",
    "        return dataset_path\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Dataset '{dataset_name}' not found in the current directory: {current_dir}\")\n",
    "\n",
    "# Function to read the Concode dataset (handling multiple JSON objects)\n",
    "def load_concode_dataset(dataset_path):\n",
    "    data = []\n",
    "    with open(dataset_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))  # Load each line as JSON\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line: {line}\")\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Function to clean the natural language description by removing separators (if needed)\n",
    "def clean_nl(nl):\n",
    "    return nl.replace(\"concode_field_sep\", \"|\").replace(\"concode_elem_sep\", \"->\")\n",
    "\n",
    "# Function to save results to CSV\n",
    "def save_to_csv(results, output_csv=\"GeminiResponse2025.csv\"):\n",
    "    keys = results[0].keys()\n",
    "    with open(output_csv, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "# # Function to save results to JSON\n",
    "# def save_to_json(results, output_json=\"Gemini_output.json\"):\n",
    "#     with open(output_json, \"w\") as jsonfile:\n",
    "#         json.dump(results, jsonfile, indent=4)\n",
    "\n",
    "# Function to append a single result to CSV\n",
    "def append_to_csv(result, output_csv=\"GeminiResponse2025.csv\"):\n",
    "    file_exists = os.path.exists(output_csv)\n",
    "    with open(output_csv, \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=result.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write header only if the file doesn't exist\n",
    "        writer.writerow(result)\n",
    "\n",
    "# # Function to append a single result to JSON\n",
    "# def append_to_json(result, output_json=\"Gemini_output.json\"):\n",
    "#     if os.path.exists(output_json):\n",
    "#         with open(output_json, \"r\") as jsonfile:\n",
    "#             data = json.load(jsonfile)\n",
    "#     else:\n",
    "#         data = []\n",
    "#     data.append(result)\n",
    "#     with open(output_json, \"w\") as jsonfile:\n",
    "#         json.dump(data, jsonfile, indent=4)\n",
    "\n",
    "# Main code to load dataset, generate code and save output\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Automatically locate the dataset in the current directory\n",
    "        dataset_path = locate_dataset()\n",
    "\n",
    "        # Load dataset\n",
    "        dataset = load_concode_dataset(dataset_path)\n",
    "\n",
    "        # Number of prompts to process before pausing\n",
    "        batch_size = 50  # Adjust this based on API rate limits\n",
    "        pause_duration = 60  # Pause for 60 seconds after each batch\n",
    "\n",
    "        # Iterate through the dataset and generate code for each example\n",
    "        for i, example in enumerate(dataset):\n",
    "            natural_language_desc = clean_nl(example[\"nl\"])  # Clean natural language description\n",
    "            java_code = generate_code(natural_language_desc)\n",
    "            \n",
    "            # Store the result\n",
    "            result = {\n",
    "                \"Example\": i + 1,\n",
    "                \"Generated Java Code\": java_code\n",
    "            }\n",
    "            \n",
    "            # Append the result to CSV and JSON immediately\n",
    "            append_to_csv(result)\n",
    "            # append_to_json(result)\n",
    "            \n",
    "            print(f\"Example {i + 1}:\")\n",
    "            #print(\"-\" * 50)\n",
    "\n",
    "            # Pause after each batch to avoid hitting API limits\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                print(f\"Processed {i + 1} prompts. Pausing for {pause_duration} seconds...\")\n",
    "                time.sleep(pause_duration)\n",
    "\n",
    "        print(\"All prompts processed. Results saved to CSV and JSON files.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "\n",
    "# Set the GEMINI API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDiITTsgqMxmxPMUwmwNYlaJjne9jjVs30\"  \n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])  # Configure the API key\n",
    "\n",
    "# Function to call the Gemini API for code generation\n",
    "def call_gemini_api(prompt):\n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = genai.GenerativeModel(\"gemini-2.0-flash\")  # Use the correct model name\n",
    "        # Generate content\n",
    "        response = model.generate_content(prompt)\n",
    "        # Return the generated text\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Gemini API: {e}\")\n",
    "        return \"\"  # Return an empty string or handle the error appropriately\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract code block from the response (Fallback)\n",
    "def extract_code_block(response):\n",
    "    # Use regex to find content between triple backticks\n",
    "    code_block = re.search(r'```java(.*?)```', response, re.DOTALL)\n",
    "    if code_block:\n",
    "        return code_block.group(1).strip()  # Return the code block without the backticks\n",
    "    return response.strip()  # Return the original response if no code block is found\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to generate code using the Gemini API\n",
    "def generate_code(natural_language_desc):\n",
    "    prompt = f\"\"\"\n",
    "    Generate Java code for the following task. Return ONLY the code, without any explanations, comments, or additional text.\n",
    "    Description: {natural_language_desc}\n",
    "    \"\"\"\n",
    "    response = call_gemini_api(prompt)\n",
    "    return extract_code_block(response)  # Extract and return only the code block (fallback)\n",
    "\n",
    "\n",
    "\n",
    "# Function to automatically locate the dataset\n",
    "def locate_dataset(dataset_name=\"concode/test.json\"):\n",
    "    current_dir = os.getcwd()\n",
    "    dataset_path = os.path.join(current_dir, dataset_name)\n",
    "\n",
    "    if os.path.exists(dataset_path):\n",
    "        return dataset_path\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Dataset '{dataset_name}' not found in the current directory: {current_dir}\")\n",
    "    \n",
    "\n",
    "def load_concode_dataset(dataset_path):\n",
    "    data = []\n",
    "    with open(dataset_path, \"r\", encoding='utf-8') as file:  # Add encoding='utf-8'\n",
    "        for line in file:\n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line: {line}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error parsing JSON: {e}\")\n",
    "                continue\n",
    "    return data   \n",
    "\n",
    "\n",
    "\n",
    "# Function to clean the natural language description by removing separators (if needed)\n",
    "def clean_nl(nl):\n",
    "    return nl.replace(\"concode_field_sep\", \"|\").replace(\"concode_elem_sep\", \"->\")\n",
    "\n",
    "# Function to append a single result to CSV\n",
    "def append_to_csv(result, output_csv=\"GeminiResponse2025.csv\"):\n",
    "    file_exists = os.path.exists(output_csv)\n",
    "    with open(output_csv, \"a\", newline=\"\", encoding='utf-8') as csvfile:  # Add encoding='utf-8'\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=result.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(result)\n",
    "\n",
    "\n",
    "\n",
    "# Main code to load dataset, generate code and save output\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        dataset_path = locate_dataset()\n",
    "        dataset = load_concode_dataset(dataset_path)\n",
    "        batch_size = 50\n",
    "        pause_duration = 60\n",
    "\n",
    "        for i, example in enumerate(dataset):\n",
    "            try:\n",
    "                natural_language_desc = clean_nl(example[\"nl\"])  # Wrap this in try-except\n",
    "                java_code = generate_code(natural_language_desc)\n",
    "\n",
    "                result = {\n",
    "                    \"Example\": i + 1,\n",
    "                    \"Generated Java Code\": java_code\n",
    "                }\n",
    "\n",
    "                append_to_csv(result)\n",
    "\n",
    "                print(f\"Example {i + 1}:\")\n",
    "\n",
    "                if (i + 1) % batch_size == 0:\n",
    "                    print(f\"Processed {i + 1} prompts. Pausing for {pause_duration} seconds...\")\n",
    "                    time.sleep(pause_duration)\n",
    "            except KeyError as e:\n",
    "                print(f\"Error: Missing 'nl' field in example {i + 1}. Skipping this example.\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error processing Example {i + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(\"All prompts processed. Results saved to CSV files.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved as Output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "input_file = \"GeminiResponse2025.csv\"  # Replace with your file name\n",
    "output_file = \"Output.csv\"  # Output file name\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Drop rows where the 'txt' column is empty\n",
    "df = df.dropna(subset=['Generated Java Code'])  # Drop rows with empty 'txt' column\n",
    "\n",
    "# Reset the 'Serial No' column to sequential numbers\n",
    "df['Example'] = range(1, len(df) + 1)\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Cleaned file saved as {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
