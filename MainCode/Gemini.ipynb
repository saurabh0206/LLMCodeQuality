{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set the GEMINI API key\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyAF4pc3g0hhNEncHEiPbqSZLBfRfGXlGPo\"\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Function to call the Gemini API for code generation\n",
    "def call_gemini_api(prompt):\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return \"\"  # Return an empty string or handle the error appropriately\n",
    "\n",
    "# Function to generate code using the Gemini API\n",
    "def generate_code(natural_language_desc):\n",
    "    prompt = f\"Generate Java code for the following task:\\nDescription: {natural_language_desc}\"\n",
    "    return call_gemini_api(prompt)\n",
    "\n",
    "\n",
    "# Function to automatically locate the dataset\n",
    "def locate_dataset(dataset_name=\"concode/test.json\"):\n",
    "    current_dir = os.getcwd()\n",
    "    dataset_path = os.path.join(current_dir, dataset_name)\n",
    "    \n",
    "    if os.path.exists(dataset_path):\n",
    "        return dataset_path\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Dataset '{dataset_name}' not found in the current directory: {current_dir}\")\n",
    "\n",
    "# Function to read the Concode dataset (handling multiple JSON objects)\n",
    "def load_concode_dataset(dataset_path):\n",
    "    data = []\n",
    "    with open(dataset_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))  # Load each line as JSON\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line: {line}\")\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Function to clean the natural language description by removing separators (if needed)\n",
    "def clean_nl(nl):\n",
    "    return nl.replace(\"concode_field_sep\", \"|\").replace(\"concode_elem_sep\", \"->\")\n",
    "\n",
    "# Function to generate code using OpenAI\n",
    "\n",
    "\n",
    "# Function to save results to CSV\n",
    "def save_to_csv(results, output_csv=\"Gemini_output.csv\"):\n",
    "    keys = results[0].keys()\n",
    "    with open(output_csv, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "# Function to save results to JSON\n",
    "def save_to_json(results, output_json=\"Gemini_output.json\"):\n",
    "    with open(output_json, \"w\") as jsonfile:\n",
    "        json.dump(results, jsonfile, indent=4)\n",
    "\n",
    "# Main code to load dataset, generate code and save output\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Automatically locate the dataset in the current directory\n",
    "        dataset_path = locate_dataset()\n",
    "\n",
    "        # Load dataset\n",
    "        dataset = load_concode_dataset(dataset_path)\n",
    "\n",
    "        # Limit to 10 data items\n",
    "        dataset_sample = dataset[:200]\n",
    "\n",
    "        # List to store results\n",
    "        results = []\n",
    "\n",
    "        # Iterate through the dataset sample and generate code for each example\n",
    "        for i, example in enumerate(dataset_sample):\n",
    "            natural_language_desc = clean_nl(example[\"nl\"])  # Clean natural language description\n",
    "            java_code = generate_code(natural_language_desc)\n",
    "            # java_code = generate_code(\"is input number is even or odd\")\n",
    "            \n",
    "            # Store the result\n",
    "            result = {\n",
    "                \"Example\": i + 1,\n",
    "                # \"Description\": natural_language_desc,\n",
    "                \"Generated Java Code\": java_code\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            save_to_json(results, output_json=\"Gemini_output.json\")\n",
    "            print(f\"Example {i + 1}:\")\n",
    "            # print(f\"Description: {natural_language_desc}\")\n",
    "            # print(f\"Generated Java Code:\\n{java_code}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "\n",
    "        # Save the results to CSV and JSON\n",
    "        # save_to_csv(results)\n",
    "        # save_to_json(results)\n",
    "\n",
    "        print(\"Results saved to CSV and JSON files.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(results)\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till API Exhausted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'concode/test.json' not found in the current directory: c:\\Users\\ULTRON\\OneDrive\\Desktop\\Project phase 4\\Implimentation\\MainCode\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set the GEMINI API key\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyCNi6stkF_6Wks1cpS-Op1rcZsltKo4vcI\"  # Replace with your actual API key\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Function to call the Gemini API for code generation\n",
    "def call_gemini_api(prompt):\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Gemini API: {e}\")\n",
    "        return \"\"  # Return an empty string or handle the error appropriately\n",
    "\n",
    "# Function to generate code using the Gemini API\n",
    "def generate_code(natural_language_desc):\n",
    "    prompt = f\"Generate Java code for the following task:\\nDescription: {natural_language_desc}\"\n",
    "    return call_gemini_api(prompt)\n",
    "\n",
    "# Function to automatically locate the dataset\n",
    "def locate_dataset(dataset_name=\"concode/test.json\"):\n",
    "    current_dir = os.getcwd()\n",
    "    dataset_path = os.path.join(current_dir, dataset_name)\n",
    "    \n",
    "    if os.path.exists(dataset_path):\n",
    "        return dataset_path\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Dataset '{dataset_name}' not found in the current directory: {current_dir}\")\n",
    "\n",
    "# Function to read the Concode dataset (handling multiple JSON objects)\n",
    "def load_concode_dataset(dataset_path):\n",
    "    data = []\n",
    "    with open(dataset_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))  # Load each line as JSON\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line: {line}\")\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Function to clean the natural language description by removing separators (if needed)\n",
    "def clean_nl(nl):\n",
    "    return nl.replace(\"concode_field_sep\", \"|\").replace(\"concode_elem_sep\", \"->\")\n",
    "\n",
    "# Function to save results to CSV\n",
    "def save_to_csv(results, output_csv=\"GeminiResponse2025.csv\"):\n",
    "    keys = results[0].keys()\n",
    "    with open(output_csv, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "# # Function to save results to JSON\n",
    "# def save_to_json(results, output_json=\"Gemini_output.json\"):\n",
    "#     with open(output_json, \"w\") as jsonfile:\n",
    "#         json.dump(results, jsonfile, indent=4)\n",
    "\n",
    "# Function to append a single result to CSV\n",
    "def append_to_csv(result, output_csv=\"GeminiResponse2025.csv\"):\n",
    "    file_exists = os.path.exists(output_csv)\n",
    "    with open(output_csv, \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=result.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write header only if the file doesn't exist\n",
    "        writer.writerow(result)\n",
    "\n",
    "# # Function to append a single result to JSON\n",
    "# def append_to_json(result, output_json=\"Gemini_output.json\"):\n",
    "#     if os.path.exists(output_json):\n",
    "#         with open(output_json, \"r\") as jsonfile:\n",
    "#             data = json.load(jsonfile)\n",
    "#     else:\n",
    "#         data = []\n",
    "#     data.append(result)\n",
    "#     with open(output_json, \"w\") as jsonfile:\n",
    "#         json.dump(data, jsonfile, indent=4)\n",
    "\n",
    "# Main code to load dataset, generate code and save output\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Automatically locate the dataset in the current directory\n",
    "        dataset_path = locate_dataset()\n",
    "\n",
    "        # Load dataset\n",
    "        dataset = load_concode_dataset(dataset_path)\n",
    "\n",
    "        # Number of prompts to process before pausing\n",
    "        batch_size = 50  # Adjust this based on API rate limits\n",
    "        pause_duration = 60  # Pause for 60 seconds after each batch\n",
    "\n",
    "        # Iterate through the dataset and generate code for each example\n",
    "        for i, example in enumerate(dataset):\n",
    "            natural_language_desc = clean_nl(example[\"nl\"])  # Clean natural language description\n",
    "            java_code = generate_code(natural_language_desc)\n",
    "            \n",
    "            # Store the result\n",
    "            result = {\n",
    "                \"Example\": i + 1,\n",
    "                \"Generated Java Code\": java_code\n",
    "            }\n",
    "            \n",
    "            # Append the result to CSV and JSON immediately\n",
    "            append_to_csv(result)\n",
    "            # append_to_json(result)\n",
    "            \n",
    "            print(f\"Example {i + 1}:\")\n",
    "            #print(\"-\" * 50)\n",
    "\n",
    "            # Pause after each batch to avoid hitting API limits\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                print(f\"Processed {i + 1} prompts. Pausing for {pause_duration} seconds...\")\n",
    "                time.sleep(pause_duration)\n",
    "\n",
    "        print(\"All prompts processed. Results saved to CSV and JSON files.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
